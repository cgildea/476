\def\chapternumber{Chapter 16 --- Concurrent Server Issues -- 16 January 2001}
\input slides.tex
\centerline{\bbf Chapter 16}
\centerline{Concurrent Server Issues}

{\bit Design Choice:} Iterative vs. Concurrent

Concurrent (Multitasking): easier to program because
\break
each process handles one client.

More CPU because creating processes and context switching takes resources.
\vt
Iterative: Most efficient throughput.

Inappropriate for long interactions because
\break
one connection blocks all others.
\vt
Concurrent (Single process): efficient because it avoids creating
processes and context switching.

Very difficult to program because a single procedure has to keep track 
of what lots of clients are doing, and keep them straight.

\vt
Unix has found a multiple process approach useful.

Concurrency wins on multi-processor machines
\newslide
{\bit Concurrency Issue:}

How many processes should be allowed to run at a time
(level of concurrency)

Usually demand driven with an upper limit.

{\bit Concurrency Cost}

Time required to create a process, $c$

Time required to process a request, $p$

If $p < c$, then concurrency looses, even with two requests:

\ind{}{\bit Concurrent:} $3c/2 + p$ time
\ind{}{\bit Iterative:} $3p/2$ time

Under heavy load, the concurrent server fails before the iterative
(on a single processor).
\vt
If $p>>c$ and the client does lots of waiting during the interaction.
This causes a long delay if you have an iterative server and
more than one client.
\newslide
\centerline{\bbf Process Preallocation}

Concept: Preallocate concurrent processes to avoid the
cost of creating them on demand.

Master server preallocates N processes at start-up.
\break
Each process waits for requests to arrive.
\break
When a request arrives, one process handles it.

Result: Lower OS overhead

Multiserver coordination:
Sometimes shared memory or message passing is available .

UNIX:

\ind{--} shared sockets (children inherit descriptors)
\ind{--} mutual exclusion for multiple processes 
trying to accept a connection from the same socket 
\ind{--} only one process gets the message

Connection Oriented: one server accepts the connection.

Conncetionless: All server processes use the same port,
one gets the message and responds.

Example (almost): {\ltt{}nfsd} (network file system servers)
\newslide
\centerline{\bbf Delayed Process Allocation}

Concept: Sometimes efficiency is improved by delaying process allocation

Recall: Concurrency works when the cost of creating a
process is smaller than the cost of processing a request 

Problem: The processing time may vary with the request.

Solution: Use delayed process allocation 

Master server receives a request, starts a
timer to measure elapsed time, and begins
processing the request (iteratively)

If the timer expires before the server is through 
processing, the server creates a slave process 
to handle the rest of the request 

Else, the master cancels the timer when the request is through.

\newslide
{\bit Combining Preallocation and Delayed Allocation}

(similar to what {\ltt{}httpd} does)
\vt
Both approaches use the same principle: 

decouple the level of
server concurrency from the number of currently active requests
\vt

The two ideas can both be combined:

Create new processes based on delayed process
allocation technique 
\break
BUT leave the processes active after they finish processing,
waiting for the next incoming request 

Difficulty: How do you know when a slave process should exit?
\bye
